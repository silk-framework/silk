<configuration>

  <property name="elds.home" value="${ELDS_HOME:-${user.home}/.elds}"/>

  <appender name="FILE" class="ch.qos.logback.core.FileAppender">
    <file>${elds.home}/var/log/dataintegration.log</file>
    <encoder>
      <pattern>%d{yyyy-MM-dd'T'HH:mm:ss.SSSX,UTC} %-5level %logger{15} - %message%n%xException</pattern>
    </encoder>
  </appender>

  <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
    <encoder>
      <pattern>%d{yyyy-MM-dd'T'HH:mm:ss.SSSX,UTC} %-5level %logger{15} - %message%n%xException</pattern>
    </encoder>
  </appender>

  <!-- Valid logging levels: ERROR, WARN, INFO, DEBUG, TRACE -->
  <!-- # RDF Plugin related logging (e.g. queries) -->
  <!--<logger name="org.silkframework.plugins.dataset.rdf" level="TRACE" />-->

  <appender name="sparkFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>/tmp/spark.log</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <!-- rotate every day -->
      <fileNamePattern>/spark.%d{yyyyMMdd}.log.zip</fileNamePattern>
    </rollingPolicy>
    <encoder>
      <pattern>%d{yyyy-MM-dd'T'HH:mm:ss.SSSX,UTC} %-5level %logger{15} - %message%n%xException</pattern>
    </encoder>
  </appender>


  <root level="info">
    <appender-ref ref="STDOUT"/>
    <appender-ref ref="FILE"/>
  </root>

  <!-- OAuth specific log messages  -->
  <logger name="oauth" level="info" additivity="false">
    <appender-ref ref="STDOUT"/>
    <appender-ref ref="FILE"/>
  </logger>


  <logger name="category.DataNucleus" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.hadoop.util.Shell" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.parquet" level="warn" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.executor" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.storage" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.scheduler" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.SparkContext" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.CacheManager" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.MapOutputTracker" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.MapOutputTrackerMaster" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.ContextCleaner" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.rdd" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.ClosureCleaner" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.spark-project.jetty" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.hadoop" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.sql.execution.datasources.parquet" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.sql.catalyst.expressions.codegen" level="info" additivity="false">
    <appender-ref ref="sparkFileAppender"/>
  </logger>
</configuration>
