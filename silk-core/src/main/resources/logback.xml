<configuration>

  <property name="elds.home" value="${ELDS_HOME:-${user.home}/.elds}"/>

  <appender name="FILE" class="ch.qos.logback.core.FileAppender">
    <file>${elds.home}/var/log/dataintegration.log</file>
    <encoder>
      <pattern>%date - [%level] - from %logger in %thread %n%message%n%xException%n</pattern>
    </encoder>
  </appender>

  <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
    <encoder>
      <pattern>%date %highlight(%-5level) %logger{15} %message%n%xException</pattern>
    </encoder>
  </appender>

  <!-- Valid logging levels: ERROR, WARN, INFO, DEBUG, TRACE -->
  <!-- # RDF Plugin related logging (e.g. queries) -->
  <!--<logger name="org.silkframework.plugins.dataset.rdf" level="TRACE" />-->

  <appender name="sparkFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>/tmp/spark.log</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <!-- rotate every day -->
      <fileNamePattern>/spark.%d{yyyyMMdd}.log.zip</fileNamePattern>
    </rollingPolicy>
    <encoder>
      <pattern>%d{HH:mm:ss.SSS} %-5level %logger{100} - %msg%n</pattern>
    </encoder>
  </appender>


  <root level="info">
    <appender-ref ref="STDOUT"/>
    <appender-ref ref="FILE"/>
  </root>

  <!-- OAuth specific log messages  -->
  <logger name="oauth" additivity="false">
    <level value="info"/>
    <appender-ref ref="STDOUT"/>
    <appender-ref ref="FILE"/>
  </logger>


  <logger name="category.DataNucleus" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.hadoop.util.Shell" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.parquet" additivity="false">
    <level value="warn"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.executor" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.storage" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.scheduler" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.SparkContext" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.CacheManager" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.MapOutputTracker" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.MapOutputTrackerMaster"
          additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.ContextCleaner" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.rdd" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.ClosureCleaner" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.spark-project.jetty" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.hadoop" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.sql.execution.datasources.parquet" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>

  <logger name="org.apache.spark.sql.catalyst.expressions.codegen" additivity="false">
    <level value="info"/>
    <appender-ref ref="sparkFileAppender"/>
  </logger>
</configuration>
